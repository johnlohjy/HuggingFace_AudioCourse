{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading Datasets\n",
    "\n",
    "Combine the train and validation splits of the Common Voice 13 Dhivehi dataset to be the training data\n",
    "\n",
    "```\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)\n",
    "```\n",
    "\n",
    "```\n",
    "output:\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
    "        num_rows: 4904\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
    "        num_rows: 2212\n",
    "    })\n",
    "})\n",
    "```\n",
    "\n",
    "Pick only the columns we need: Audio Samples and their correpsponding transcribed text\n",
    "\n",
    "```\n",
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extractor, Tokenizer -> Processor\n",
    "\n",
    "3 stages of the ASR pipeline\n",
    "\n",
    "1. The feature extractor which pre-processes the raw audio-inputs to log-mel spectrograms\n",
    "\n",
    "2. The model which performs the sequence-to-sequence mapping\n",
    "\n",
    "3. The tokenizer which post-processes/maps the predicted tokens to text\n",
    "\n",
    "For HuggingFace Transformers, the Whisper model has an associated feature extractor and tokenizer\n",
    "- WhisperFeatureExtractor\n",
    "- WhisperTokenizer\n",
    "\n",
    "Conveniently, these objects are wrapped under a single class called the WhisperProcessor\n",
    "- We can call the WhisperProcessor to perform both the audio pre-processing and the text token post-processing. \n",
    "\n",
    "In doing so, we only need to keep track of two objects during training: the processor and the model.\n",
    "\n",
    "When performing multilingual fine-tuning, we need to set the \"language\" and \"task\" when instantiating the processor. \n",
    "- \"language\" should be set to the source audio language\n",
    "- task to \"transcribe\" for speech recognition or \"translate\" for speech translation \n",
    "These arguments modify the behaviour of the tokenizer, and should be set correctly to ensure the target labels are encoded properly.\n",
    "\n",
    "See all possible languages supported by Whisper\n",
    "\n",
    "```\n",
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "\n",
    "TO_LANGUAGE_CODE\n",
    "```\n",
    "\n",
    "Dhivehi is not a supported language: Whisper was not pre-trained on it. But we can fine-tune Whisper on it i.e teach it a new language, one that the pre-trained checkpoint does not support\n",
    "\n",
    "When you fine-tune it on a new language, Whisper does a good job at leveraging its knowledge of the other 96 languages it’s pre-trained on. Largely speaking, all modern languages will be linguistically similar to at least one of the 96 languages Whisper already knows, so we’ll fall under this paradigm of cross-lingual knowledge representation.\n",
    "- What we need to do to fine-tune Whisper on a new language is find the language most similar that Whisper was pre-trained on. The Wikipedia article for Dhivehi states that Dhivehi is closely related to the Sinhalese language of Sri Lanka. If we check the language codes again, we can see that Sinhalese is present in the Whisper language set, so we can safely set our language argument to \"sinhalese\".\n",
    "\n",
    "Load the Processor that contains the feature extractor and tokenizer\n",
    "```\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pre-process the Data\n",
    "\n",
    "Look at the dataset features\n",
    "\n",
    "```\n",
    "common_voice[\"train\"].features\n",
    "\n",
    "output:\n",
    "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
    " 'sentence': Value(dtype='string', id=None)}\n",
    "```\n",
    "\n",
    "The input audio is sampled at 48kHz. We need to downsample it to 16kHz before passing it to the Whisper feature extractor because the Whisper model expects a sample rate of 16kHz\n",
    "\n",
    "### Resample on the fly\n",
    "```\n",
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "\n",
    "\"\"\"\n",
    "Set the audio inputs to the correct sampling rate using dataset's cast_column method\n",
    "does not change audio in-place but rather, signals to datasets to resample audio samples on-the-fly when loaded\n",
    "\"\"\"\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "```\n",
    "\n",
    "Additionally, we write a function to prepare our data for the model:\n",
    "\n",
    "### Write function that gets input features (log mel spectrogram) and output labels\n",
    "```\n",
    "def prepare_dataset(example):\n",
    "    # Load the audio which is resampled on the fly\n",
    "    # Hugging Face datasets library performs resampling operations on the fly\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    # Processor is a combination of feature extractor and tokenizer\n",
    "    # Compute the log-mel spectrogram input feature from 1D audio array\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        # Possibly uses this to encode it into label IDs (tokenizer bit)\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # The input features (log mel spectrogram) and output labels should be stored in example\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example\n",
    "```\n",
    "\n",
    "### Apply function and drop cols\n",
    "We then apply this data preparation to all of our training examples\n",
    "Remove the audio and text col of the raw training data, leaving just\n",
    "the cols returned by the prepare_dataset function\n",
    "\n",
    "```\n",
    "\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")\n",
    "```\n",
    "\n",
    "### Filter training data with audio samples longer than 30s\n",
    "\n",
    "These samples would otherwise be truncated by the Whisper feature-extractor which could affect the stability of training. We define a function that returns True for samples that are less than 30s, and False for those that are longer\n",
    "\n",
    "```\n",
    "max_input_length = 30.0\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "```\n",
    "\n",
    "Apply the filter function to all samples of our training dataset through the HuggingFace Dataset's .filter method\n",
    "\n",
    "```\n",
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "common_voice[\"train\"]\n",
    "\n",
    "output:\n",
    "Dataset({\n",
    "    features: ['input_features', 'labels', 'input_length'],\n",
    "    num_rows: 4904\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training and Evaluation\n",
    "\n",
    "#### Define a Data Collator\n",
    "\n",
    "The data collator takes our pre-processed data and prepares PyTorch tensors ready for the model\n",
    "\n",
    "It treats the ```input_features``` and ```labels``` independently\n",
    "- ```input_features``` is handled by the feature extractor\n",
    "- ```labels``` is handled by the tokenizer\n",
    "\n",
    "```input_features``` has already been padded to 30s and converted to log-mel spectrogram of a fixed dimension. We just need to convert them to batched PyTorch tensors.\n",
    "- Do this using feature extractor's ```.pad``` method with ```return_tensors=pt``` (pytorch). No additional padding is actually applied here, the ```input_features``` are simply convered to PyTorch tensors\n",
    "\n",
    "```labels``` are unpadded. So, we first pad the sequences to the max length in the batch using the tokenizer's ```.pad``` method. \n",
    "-  The padding tokens are then replaced by -100 so that these tokens are not taken into account when computing the loss. We then cut the start of transcript token from the beginning of the label sequence as we append it later during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    # Allows an instance of the class to be called as a function\n",
    "    # Takes a list of features, each a dictionary, and returns a dictionary of tensors\n",
    "    # ready for model input\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        \n",
    "        \"\"\"\n",
    "        Prepare the input_features\n",
    "\n",
    "        For each feature,\n",
    "            extract out the log-mel spectrogram \n",
    "\n",
    "        Pad the input_features as pytorchtensors - a list of dictionaries, each containing a single key \"input_features\" with the padded audio data\n",
    "        \"\"\"\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        \"\"\"\n",
    "        Prepare the labels\n",
    "\n",
    "        For each label,\n",
    "            extract out the labels\n",
    "\n",
    "        Pad the label_features as pytorchtensors - a list of dictionaries, each containing a single key \"input_ids\" with the padded label seq \n",
    "        \"\"\"\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        \"\"\"\n",
    "        Replace the padding tokens in the labels with -100\n",
    "        during training, many loss functions in PyTorch ignore targets with the value -100. This ensures that the padding tokens do not contribute to the loss calculation.\n",
    "        \n",
    "        attention_mask is a tensor that typically indicates which positions in the input sequence should be attended to by the model.\n",
    "        - 1s: actual data\n",
    "        - 0s: padding tokens\n",
    "\n",
    "        .ne(1)->not equal 1, returns a new tensor of the same shape, where each element is True if the corresponding element in the original tensor is not equal to 1, and False otherwise.\n",
    "        \"\"\"\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        labels[:, 0] is a 2D tensor, extracts first token from each seq\n",
    "\n",
    "        .all() -> return T if every element in the boolean tensor is T\n",
    "\n",
    "        .cpu().item() -> convert result from tensor to bool\n",
    "        \"\"\"\n",
    "        # if bos (beginning of sequence) token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the WER\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Extract predictions (predicted token IDs) by the model\n",
    "    # Extract reference/ground truth token ids\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id -> -100 was used so they were igored in loss calculation\n",
    "    # revert to actual padding token ID when using WER for evaluation\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    # Decode the predicted and label ids to strings\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic wer\n",
    "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    \n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Pre-trained Checkpoint/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Checkpointing\n",
    "\n",
    "Saves only a SUBSET/checkpoint of the activations/outputs that are used for calculating backpropagation\n",
    "- recomputes the rest of the activations/intermediate otputs during backpropagation\n",
    "\n",
    "Save a lot memory by storing fewwer activation\n",
    "\n",
    "Caching refers to storing the activations during the forward pass\n",
    "\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# partical -> partial function from the functools module is used to create a new version of a function with some arguments fixed. In this case, it's being used to modify the generate method of the model\n",
    "# -> override the generate method of the model so that it always includes certain arguments when it's called\n",
    "# set language and task for generation and re-enable cache\n",
    "# caching is re-enabled because its needed for inference\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the training configuration\n",
    "\n",
    "Define all parameters related to training\n",
    "\n",
    "Training arguments:\n",
    "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-dv\",  # name on the HF Hub\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward the training arguments to the HuggingFace Trainer along with the\n",
    "- Model\n",
    "- Dataset\n",
    "- Data Collator\n",
    "- Compute Metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Output Performance"
   ]
  },
  {
   "attachments": {
    "Fine-Tune Results.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fine-Tune Results.PNG](<attachment:Fine-Tune Results.PNG>)\n",
    "\n",
    "Our final WER is 14.1% - not bad for seven hours of training data and just 500 training steps! That amounts to a 112% improvement versus the pre-trained model! That means we’ve taken a model that previously had no knowledge about Dhivehi, and fine-tuned it to recognise Dhivehi speech with adequate accuracy in under one hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
